{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8f80603-334e-4f83-a6da-5ab31e109256",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add parent directory to path: enable import from parent dir\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "from agents.sac_disc import SAC\n",
    "import pybullet_envs\n",
    "import gym_algorithmic\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43bac3fb-1e10-49fc-b633-cca64f349daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1376f37-6637-4eae-85c8-1ebafe1a6f24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(2)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "603f1b52-461d-45a4-87ed-18ff6aa816d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sac = SAC(\n",
    "    env=env,\n",
    "    name='cartpole_discrete',\n",
    "    input_dim=env.observation_space.shape[0],\n",
    "    log_freq=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1aae9d16-1111-45e6-bcf4-d2bde0452e24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "collecting experience...\n",
      "0..10..20..30..40..50..60..70.."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vinay/code/git/reinforcement-learning-algos/notebooks/../agents/sac_disc.py:315: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  state = T(state, device=DEVICE)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80..90..100..110..120..130..140..150..160..170..180..190..200..210..220.."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vinay/code/git/reinforcement-learning-algos/notebooks/../agents/sac_disc.py:250: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  rewards = T(rewards, dtype=torch.float, device=DEVICE)\n",
      "/Users/vinay/code/git/reinforcement-learning-algos/notebooks/../agents/sac_disc.py:251: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  dones = T(dones, dtype=torch.float, device=DEVICE)\n",
      "/Users/vinay/code/git/reinforcement-learning-algos/notebooks/../agents/sac_disc.py:252: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  nxt_states = T(nxt_states, dtype=torch.float, device=DEVICE)\n",
      "/Users/vinay/code/git/reinforcement-learning-algos/notebooks/../agents/sac_disc.py:253: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  states = T(states, dtype=torch.float, device=DEVICE)\n",
      "/Users/vinay/code/git/reinforcement-learning-algos/notebooks/../agents/sac_disc.py:254: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  actions = T(actions, dtype=torch.float, device=DEVICE)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 230, Reward: 38.0, Avg. Reward: 22.6, Policy Loss=-0.7\n",
      "Episode: 240, Reward: 10.0, Avg. Reward: 22.98, Policy Loss=-3.42\n",
      "Episode: 250, Reward: 16.0, Avg. Reward: 24.4, Policy Loss=-6.95\n",
      "Episode: 260, Reward: 78.0, Avg. Reward: 24.76, Policy Loss=-10.39\n",
      "Episode: 270, Reward: 84.0, Avg. Reward: 30.14, Policy Loss=-19.69\n",
      "Episode: 280, Reward: 172.0, Avg. Reward: 44.68, Policy Loss=-54.92\n",
      "Episode: 290, Reward: 200.0, Avg. Reward: 64.18, Policy Loss=-143.57\n",
      "Episode: 300, Reward: 170.0, Avg. Reward: 91.62, Policy Loss=-429.87\n",
      "Episode: 310, Reward: 129.0, Avg. Reward: 108.94, Policy Loss=-820.32\n",
      "Episode: 320, Reward: 200.0, Avg. Reward: 118.94, Policy Loss=-1419.32\n",
      "Episode: 330, Reward: 47.0, Avg. Reward: 114.14, Policy Loss=-2229.17\n",
      "Episode: 340, Reward: 197.0, Avg. Reward: 113.34, Policy Loss=-4605.68\n",
      "Episode: 350, Reward: 24.0, Avg. Reward: 96.12, Policy Loss=-7683.13\n",
      "Episode: 360, Reward: 33.0, Avg. Reward: 88.6, Policy Loss=-12921.09\n",
      "Episode: 370, Reward: 70.0, Avg. Reward: 79.22, Policy Loss=-18845.55\n",
      "Episode: 380, Reward: 151.0, Avg. Reward: 77.16, Policy Loss=-28669.04\n",
      "Episode: 390, Reward: 77.0, Avg. Reward: 65.12, Policy Loss=-39244.29\n",
      "Episode: 400, Reward: 14.0, Avg. Reward: 57.62, Policy Loss=-48582.46\n",
      "Episode: 410, Reward: 78.0, Avg. Reward: 50.98, Policy Loss=-60264.99\n",
      "Episode: 420, Reward: 69.0, Avg. Reward: 48.5, Policy Loss=-73390.92\n",
      "Episode: 430, Reward: 23.0, Avg. Reward: 45.98, Policy Loss=-91616.34\n",
      "Episode: 440, Reward: 77.0, Avg. Reward: 43.76, Policy Loss=-107399.96\n",
      "Episode: 450, Reward: 19.0, Avg. Reward: 41.46, Policy Loss=-118087.86\n",
      "Episode: 460, Reward: 56.0, Avg. Reward: 42.06, Policy Loss=-136267.94\n",
      "Episode: 470, Reward: 30.0, Avg. Reward: 41.56, Policy Loss=-154161.2\n",
      "Episode: 480, Reward: 17.0, Avg. Reward: 39.9, Policy Loss=-174919.75\n",
      "Episode: 490, Reward: 20.0, Avg. Reward: 36.82, Policy Loss=-187668.97\n",
      "Episode: 500, Reward: 34.0, Avg. Reward: 42.26, Policy Loss=-215680.77\n",
      "Episode: 510, Reward: 118.0, Avg. Reward: 41.64, Policy Loss=-238683.98\n",
      "Episode: 520, Reward: 65.0, Avg. Reward: 39.18, Policy Loss=-253274.31\n",
      "Episode: 530, Reward: 21.0, Avg. Reward: 38.46, Policy Loss=-276816.53\n",
      "Episode: 540, Reward: 52.0, Avg. Reward: 41.48, Policy Loss=-302561.56\n",
      "Episode: 550, Reward: 36.0, Avg. Reward: 38.62, Policy Loss=-327313.62\n",
      "Episode: 560, Reward: 31.0, Avg. Reward: 38.52, Policy Loss=-355524.03\n",
      "Episode: 570, Reward: 72.0, Avg. Reward: 44.22, Policy Loss=-396355.62\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/78/_bz_v2_103ld252mxn9ml9c00000gn/T/ipykernel_88354/79452529.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msac\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/code/git/reinforcement-learning-algos/notebooks/../agents/sac_disc.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, ep)\u001b[0m\n\u001b[1;32m    321\u001b[0m                 \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnxt_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurr_size\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmem_sz\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 323\u001b[0;31m                     \u001b[0ma_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mep_no\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mep_no\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/code/git/reinforcement-learning-algos/notebooks/../agents/sac_disc.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, ep_no)\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[0mcurr_alpha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m         policy_loss, log_probs = self._get_policy_loss(\n\u001b[0m\u001b[1;32m    258\u001b[0m             \u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m             \u001b[0mcurr_alpha\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/code/git/reinforcement-learning-algos/notebooks/../agents/sac_disc.py\u001b[0m in \u001b[0;36m_get_policy_loss\u001b[0;34m(self, states, alpha)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_policy_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m             \u001b[0mpred_q1_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcritic_a\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/code/git/reinforcement-learning-algos/notebooks/../networks/sac/discrete/policy_net.py\u001b[0m in \u001b[0;36msample\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprobs\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m1e-8\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0mlog_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_probs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "sac.run(1000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
