{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8f80603-334e-4f83-a6da-5ab31e109256",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add parent directory to path: enable import from parent dir\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "from agents.sac_disc import SAC\n",
    "import pybullet_envs\n",
    "import gym_algorithmic\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43bac3fb-1e10-49fc-b633-cca64f349daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1376f37-6637-4eae-85c8-1ebafe1a6f24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(2)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "603f1b52-461d-45a4-87ed-18ff6aa816d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sac = SAC(\n",
    "    env=env,\n",
    "    name='cartpole_discrete',\n",
    "    input_dim=env.observation_space.shape[0],\n",
    "    log_freq=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1aae9d16-1111-45e6-bcf4-d2bde0452e24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "collecting experience...\n",
      "0..10..20..30..40..50..60..70.."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vinay/code/git/reinforcement-learning-algos/notebooks/../agents/sac_disc.py:330: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  state = T(state, device=DEVICE)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80..90..100..110..120..130..140..150..160..170..180..190..200.."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vinay/code/git/reinforcement-learning-algos/notebooks/../agents/sac_disc.py:261: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  rewards = T(rewards, dtype=torch.float, device=DEVICE)\n",
      "/Users/vinay/code/git/reinforcement-learning-algos/notebooks/../agents/sac_disc.py:262: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  dones = T(dones, dtype=torch.float, device=DEVICE)\n",
      "/Users/vinay/code/git/reinforcement-learning-algos/notebooks/../agents/sac_disc.py:263: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  nxt_states = T(nxt_states, dtype=torch.float, device=DEVICE)\n",
      "/Users/vinay/code/git/reinforcement-learning-algos/notebooks/../agents/sac_disc.py:264: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  states = T(states, dtype=torch.float, device=DEVICE)\n",
      "/Users/vinay/code/git/reinforcement-learning-algos/notebooks/../agents/sac_disc.py:265: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  actions = T(actions, dtype=torch.float, device=DEVICE)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 210, Reward: 20.0, Avg. Reward: 20.7, Policy Loss=1.73\n",
      "Episode: 220, Reward: 11.0, Avg. Reward: 21.56, Policy Loss=5.9\n",
      "Episode: 230, Reward: 40.0, Avg. Reward: 24.72, Policy Loss=10.66\n",
      "Episode: 240, Reward: 13.0, Avg. Reward: 25.1, Policy Loss=13.27\n",
      "Episode: 250, Reward: 15.0, Avg. Reward: 25.18, Policy Loss=16.48\n",
      "Episode: 260, Reward: 53.0, Avg. Reward: 25.74, Policy Loss=18.7\n",
      "Episode: 270, Reward: 44.0, Avg. Reward: 25.36, Policy Loss=20.41\n",
      "Episode: 280, Reward: 32.0, Avg. Reward: 22.36, Policy Loss=21.69\n",
      "Episode: 290, Reward: 10.0, Avg. Reward: 22.88, Policy Loss=23.04\n",
      "Episode: 300, Reward: 40.0, Avg. Reward: 23.16, Policy Loss=23.61\n",
      "Episode: 310, Reward: 14.0, Avg. Reward: 22.62, Policy Loss=24.32\n",
      "Episode: 320, Reward: 21.0, Avg. Reward: 22.0, Policy Loss=24.73\n",
      "Episode: 330, Reward: 28.0, Avg. Reward: 21.78, Policy Loss=24.63\n",
      "Episode: 340, Reward: 10.0, Avg. Reward: 22.26, Policy Loss=24.71\n",
      "Episode: 350, Reward: 36.0, Avg. Reward: 23.6, Policy Loss=24.73\n",
      "Episode: 360, Reward: 30.0, Avg. Reward: 24.56, Policy Loss=24.58\n",
      "Episode: 370, Reward: 27.0, Avg. Reward: 24.22, Policy Loss=24.15\n",
      "Episode: 380, Reward: 23.0, Avg. Reward: 24.64, Policy Loss=24.14\n",
      "Episode: 390, Reward: 19.0, Avg. Reward: 23.06, Policy Loss=23.56\n",
      "Episode: 400, Reward: 11.0, Avg. Reward: 21.24, Policy Loss=23.54\n",
      "Episode: 410, Reward: 14.0, Avg. Reward: 19.38, Policy Loss=23.27\n",
      "Episode: 420, Reward: 45.0, Avg. Reward: 20.8, Policy Loss=22.61\n",
      "Episode: 430, Reward: 18.0, Avg. Reward: 20.98, Policy Loss=22.81\n",
      "Episode: 440, Reward: 45.0, Avg. Reward: 22.48, Policy Loss=22.4\n",
      "Episode: 450, Reward: 20.0, Avg. Reward: 22.58, Policy Loss=21.97\n",
      "Episode: 460, Reward: 14.0, Avg. Reward: 22.18, Policy Loss=21.38\n",
      "Episode: 470, Reward: 21.0, Avg. Reward: 20.74, Policy Loss=21.14\n",
      "Episode: 480, Reward: 33.0, Avg. Reward: 20.58, Policy Loss=20.72\n",
      "Episode: 490, Reward: 41.0, Avg. Reward: 19.9, Policy Loss=20.68\n",
      "Episode: 500, Reward: 18.0, Avg. Reward: 21.38, Policy Loss=20.51\n",
      "Episode: 510, Reward: 14.0, Avg. Reward: 22.14, Policy Loss=20.0\n",
      "Episode: 520, Reward: 16.0, Avg. Reward: 22.92, Policy Loss=19.64\n",
      "Episode: 530, Reward: 16.0, Avg. Reward: 23.34, Policy Loss=19.49\n",
      "Episode: 540, Reward: 10.0, Avg. Reward: 22.5, Policy Loss=19.07\n",
      "Episode: 550, Reward: 13.0, Avg. Reward: 22.04, Policy Loss=19.32\n",
      "Episode: 560, Reward: 19.0, Avg. Reward: 22.28, Policy Loss=18.69\n",
      "Episode: 570, Reward: 50.0, Avg. Reward: 22.92, Policy Loss=18.98\n",
      "Episode: 580, Reward: 40.0, Avg. Reward: 22.9, Policy Loss=18.82\n",
      "Episode: 590, Reward: 35.0, Avg. Reward: 23.5, Policy Loss=18.44\n",
      "Episode: 600, Reward: 17.0, Avg. Reward: 22.16, Policy Loss=18.5\n",
      "Episode: 610, Reward: 19.0, Avg. Reward: 22.68, Policy Loss=18.48\n",
      "Episode: 620, Reward: 23.0, Avg. Reward: 22.9, Policy Loss=18.1\n",
      "Episode: 630, Reward: 23.0, Avg. Reward: 23.64, Policy Loss=18.32\n",
      "Episode: 640, Reward: 18.0, Avg. Reward: 23.1, Policy Loss=17.8\n",
      "Episode: 650, Reward: 16.0, Avg. Reward: 22.16, Policy Loss=17.92\n",
      "Episode: 660, Reward: 44.0, Avg. Reward: 23.16, Policy Loss=18.05\n",
      "Episode: 670, Reward: 21.0, Avg. Reward: 21.44, Policy Loss=17.88\n",
      "Episode: 680, Reward: 26.0, Avg. Reward: 21.76, Policy Loss=17.89\n",
      "Episode: 690, Reward: 19.0, Avg. Reward: 23.86, Policy Loss=17.78\n",
      "Episode: 700, Reward: 29.0, Avg. Reward: 24.84, Policy Loss=17.8\n",
      "Episode: 710, Reward: 16.0, Avg. Reward: 23.24, Policy Loss=17.51\n",
      "Episode: 720, Reward: 34.0, Avg. Reward: 24.92, Policy Loss=17.76\n",
      "Episode: 730, Reward: 16.0, Avg. Reward: 23.82, Policy Loss=18.08\n",
      "Episode: 740, Reward: 20.0, Avg. Reward: 21.82, Policy Loss=17.71\n",
      "Episode: 750, Reward: 14.0, Avg. Reward: 22.96, Policy Loss=17.78\n",
      "Episode: 760, Reward: 22.0, Avg. Reward: 24.1, Policy Loss=17.95\n",
      "Episode: 770, Reward: 24.0, Avg. Reward: 23.18, Policy Loss=17.93\n",
      "Episode: 780, Reward: 31.0, Avg. Reward: 23.86, Policy Loss=17.66\n",
      "Episode: 790, Reward: 28.0, Avg. Reward: 24.82, Policy Loss=17.74\n",
      "Episode: 800, Reward: 47.0, Avg. Reward: 24.9, Policy Loss=17.84\n",
      "Episode: 810, Reward: 15.0, Avg. Reward: 23.78, Policy Loss=17.63\n",
      "Episode: 820, Reward: 43.0, Avg. Reward: 23.42, Policy Loss=17.63\n",
      "Episode: 830, Reward: 18.0, Avg. Reward: 22.46, Policy Loss=17.52\n",
      "Episode: 840, Reward: 14.0, Avg. Reward: 22.18, Policy Loss=17.44\n",
      "Episode: 850, Reward: 11.0, Avg. Reward: 20.28, Policy Loss=17.33\n",
      "Episode: 860, Reward: 27.0, Avg. Reward: 20.08, Policy Loss=17.46\n",
      "Episode: 870, Reward: 37.0, Avg. Reward: 21.28, Policy Loss=17.25\n",
      "Episode: 880, Reward: 60.0, Avg. Reward: 22.48, Policy Loss=17.29\n",
      "Episode: 890, Reward: 11.0, Avg. Reward: 22.22, Policy Loss=17.38\n",
      "Episode: 900, Reward: 14.0, Avg. Reward: 23.3, Policy Loss=17.25\n",
      "Episode: 910, Reward: 54.0, Avg. Reward: 24.0, Policy Loss=17.11\n",
      "Episode: 920, Reward: 9.0, Avg. Reward: 23.82, Policy Loss=17.26\n",
      "Episode: 930, Reward: 14.0, Avg. Reward: 22.4, Policy Loss=17.12\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected parameter probs (Tensor of shape (2,)) of distribution Categorical(probs: torch.Size([2])) to satisfy the constraint Simplex(), but found invalid values:\ntensor([nan, nan], grad_fn=<DivBackward0>)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/78/_bz_v2_103ld252mxn9ml9c00000gn/T/ipykernel_87909/79452529.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msac\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/code/git/reinforcement-learning-algos/notebooks/../agents/sac_disc.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, ep)\u001b[0m\n\u001b[1;32m    326\u001b[0m             \u001b[0mts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mep_ended\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mts\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    329\u001b[0m                 \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 \u001b[0mnxt_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mep_ended\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/code/git/reinforcement-learning-algos/notebooks/../agents/sac_disc.py\u001b[0m in \u001b[0;36m_get_action\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m    135\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m         \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/code/git/reinforcement-learning-algos/notebooks/../networks/sac/discrete/policy_net.py\u001b[0m in \u001b[0;36msample\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0maction_dist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCategorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m         \u001b[0mactions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maction_dist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprobs\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/lib/python3.9/site-packages/torch/distributions/categorical.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, probs, logits, validate_args)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_events\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_param\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mbatch_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_param\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_param\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndimension\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCategorical\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_args\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mexpand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_instance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/lib/python3.9/site-packages/torch/distributions/distribution.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch_shape, event_shape, validate_args)\u001b[0m\n\u001b[1;32m     53\u001b[0m                 \u001b[0mvalid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconstraint\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mvalid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m                     raise ValueError(\n\u001b[0m\u001b[1;32m     56\u001b[0m                         \u001b[0;34mf\"Expected parameter {param} \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m                         \u001b[0;34mf\"({type(value).__name__} of shape {tuple(value.shape)}) \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Expected parameter probs (Tensor of shape (2,)) of distribution Categorical(probs: torch.Size([2])) to satisfy the constraint Simplex(), but found invalid values:\ntensor([nan, nan], grad_fn=<DivBackward0>)"
     ]
    }
   ],
   "source": [
    "sac.run(1000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
