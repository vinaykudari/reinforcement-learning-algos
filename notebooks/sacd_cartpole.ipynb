{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8f80603-334e-4f83-a6da-5ab31e109256",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add parent directory to path: enable import from parent dir\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "from agents.sac_disc import SAC\n",
    "import pybullet_envs\n",
    "import gym_algorithmic\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43bac3fb-1e10-49fc-b633-cca64f349daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "603f1b52-461d-45a4-87ed-18ff6aa816d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sac = SAC(\n",
    "    env=env,\n",
    "    name='cartpole_discrete',\n",
    "    input_dim=env.observation_space.shape[0],\n",
    "    log_freq=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aae9d16-1111-45e6-bcf4-d2bde0452e24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "collecting experience...\n",
      "0..10..20..30..40..50.."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vinay/code/git/reinforcement-learning-algos/notebooks/../agents/sac_disc.py:318: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  state = T(state, device=DEVICE)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60..70..80..90..100..110..120..130..140..150..160..170..180..190..200..210..220..230..240..250..260..270..280..290..300..310..320..330..340..350..360..370..380..390..400..410.."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vinay/code/git/reinforcement-learning-algos/notebooks/../agents/sac_disc.py:253: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  rewards = T(rewards, dtype=torch.float, device=DEVICE)\n",
      "/Users/vinay/code/git/reinforcement-learning-algos/notebooks/../agents/sac_disc.py:254: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  dones = T(dones, dtype=torch.float, device=DEVICE)\n",
      "/Users/vinay/code/git/reinforcement-learning-algos/notebooks/../agents/sac_disc.py:255: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  nxt_states = T(nxt_states, dtype=torch.float, device=DEVICE)\n",
      "/Users/vinay/code/git/reinforcement-learning-algos/notebooks/../agents/sac_disc.py:256: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  states = T(states, dtype=torch.float, device=DEVICE)\n",
      "/Users/vinay/code/git/reinforcement-learning-algos/notebooks/../agents/sac_disc.py:257: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  actions = T(actions, dtype=torch.float, device=DEVICE)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 420, Reward: 26.0, Avg. Reward: 23.76, Policy Loss=-0.82\n",
      "Episode: 430, Reward: 21.0, Avg. Reward: 22.88, Policy Loss=-4.29\n",
      "Episode: 440, Reward: 25.0, Avg. Reward: 21.94, Policy Loss=-7.57\n",
      "Episode: 450, Reward: 20.0, Avg. Reward: 20.78, Policy Loss=-10.7\n",
      "Episode: 460, Reward: 10.0, Avg. Reward: 25.28, Policy Loss=-18.75\n",
      "Episode: 470, Reward: 168.0, Avg. Reward: 35.82, Policy Loss=-40.35\n"
     ]
    }
   ],
   "source": [
    "sac.run(1000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
